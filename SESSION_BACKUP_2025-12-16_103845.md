# Session Backup - 2025-12-16_103845

This backup contains the state of modified files and executed shell commands from the session ending on 2025-12-16_103845.

## Modified Files Content

### api/index.py
```python
import logging
import os
import re
import threading
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np  # Temporarily disabled for testing
import openai  # Temporarily disabled for testing
from fastapi import (
    BackgroundTasks,
    Body,
    FastAPI,
    File,
    Header,
    HTTPException,
    Response,
    UploadFile,
)
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from .analytics import export_analytics_csv, get_analytics_dashboard, get_analytics_health
from .competitive_intelligence import (
    analyze_company_strategic_needs,
    create_strategic_resume_targeting,
    develop_competitive_positioning_strategy,
    generate_job_search_ai_prompts,
    get_competitive_intelligence_health,
)
from .corpus_reindex import get_reindex_status, reindex_corpus
from .cost_controls import (
    check_cost_limits,
    check_resource_limits,
    get_usage_analytics,
    record_usage,
)
from .domain_adjacent_search import (
    discover_domain_adjacent_opportunities,
    get_domain_adjacent_health,
)
from .experiment_engine import (
    CapabilityEvidence,
    ExperimentCreate,
    ExperimentUpdate,
    LearningData,
    SelfEfficacyMetric,
    add_learning_data,
    capture_evidence,
    complete_experiment,
    create_experiment,
    get_experiment_health,
    get_experiments,
    get_learning_data,
    get_self_efficacy_metrics,
    record_self_efficacy_metric,
    update_experiment,
)
from .job_sources import (
    CareerBuilderSource,
    DiceSource,
    GlassdoorSource,
    GreenhouseSource,
    HackerNewsSource,
    # IndeedSource,
    LinkedInSource,
    MonsterSource,
    RedditSource,
    RemoteOKSource,
    SerpApiSource,
    # WeWorkRemotelySource,
    ZipRecruiterSource,
)
from .monitoring import attempt_system_recovery, run_health_check
from .osint_forensics import analyze_company_osint, get_osint_health
from .prompt_selector import get_prompt_health, get_prompt_response
from .ps101 import router as ps101_router
from .ps101_flow import (
    advance_ps101_step,
    create_ps101_session_data,
    exit_ps101_flow,
    format_step_for_user,
    get_completion_message,
    get_exit_confirmation,
    get_ps101_step,
)
from .ps101_flow import is_complete as ps101_is_complete
from .ps101_flow import record_ps101_response
from .rag_engine import (
    batch_compute_embeddings,
    compute_embedding,
    discover_domain_adjacent_opportunities_rag,
    get_rag_health,
    get_rag_response,
    retrieve_similar,
)
from .rag_source_discovery import (
    discover_sources_for_query,
    get_discovery_analytics,
    get_optimal_sources_for_query,
)
from .reranker import get_reranker_health
from .self_efficacy_engine import (
    cleanup_stale_experiments,
    compute_session_metrics,
    get_escalation_prompt,
    get_self_efficacy_health,
    record_analytics_entry,
    should_escalate,
)
from .settings import get_feature_flag, get_settings
from .startup_checks import startup_or_die
from .storage import (
    UPLOAD_ROOT,
    add_resume_version,
    authenticate_user,
    create_user,
    delete_session,
    diagnose_user_hash,
    ensure_session,
    fetch_job_matches,
    force_reset_user_password,
    get_conn,
    get_session_data,
    get_user_by_email,
    get_user_by_id,
    get_user_context,
    latest_metrics,
    list_resume_versions,
    record_wimd_output,
    session_exists,
    session_summary,
    store_file_upload,
    store_job_matches,
    update_job_match_status,
    update_session_data,
    wimd_history,
)

app = FastAPI()
logger = logging.getLogger(__name__)

# Booking routes removed - specs define frontend-only Google Calendar integration
# Payment system (discount codes + Stripe) not yet implemented

HEALTH_DEBUG_ENABLED = os.getenv("HEALTH_DEBUG", "").lower() in {"1", "true", "yes", "on"}
SERVICE_READY = threading.Event()

MAX_UPLOAD_BYTES = int(os.getenv("MAX_UPLOAD_BYTES", str(8 * 1024 * 1024)))
DEFAULT_METRICS = {"clarity": 0, "action": 0, "momentum": 0}
JOB_LIBRARY = [
    {
        "job_id": "delta-strategist",
        "role": "Delta Strategist",
        "company": "Mosaic Delta Lab",
        "skills": ["analysis", "storytelling", "experimentation"],
        "values": ["Innovation", "Learning"],
        "location": "Remote",
    },
    {
        "job_id": "ops-navigator",
        "role": "Operations Navigator",
        "company": "What Is My Delta",
        "skills": ["systems", "automation", "collaboration"],
        "values": ["Clarity", "Momentum"],
        "location": "Hybrid — NYC",
    },
    {
        "job_id": "coach-in-residence",
        "role": "Coach In Residence",
        "company": "Opportunity Bridge",
        "skills": ["coaching", "facilitation", "writing"],
        "values": ["Empathy", "Action"],
        "location": "Remote",
    },
    {
        "job_id": "product-scout",
        "role": "Product Scout",
        "company": "Delta Ventures",
        "skills": ["research", "product sense", "communication"],
        "values": ["Discovery", "Autonomy"],
        "location": "Remote",
    },
]

BOOKING_ROUTER_ERROR = None

# CORS configuration for Render deployment


def _build_cors_origins() -> List[str]:
    # Explicit list - don't rely on env var that might be misconfigured
    return [
        "https://whatismydelta.com",
        "https://www.whatismydelta.com",
        "https://resonant-crostata-90b706.netlify.app",
        "http://localhost:8000",  # Local development testing
        "http://127.0.0.1:8000",  # Local development testing
    ]

cors_origins = _build_cors_origins()

app.add_middleware(
    CORSMiddleware,
    allow_origins=cors_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["content-type", "authorization", "x-session-id", "x-user-id"],
    expose_headers=["*"],
)

# Include PS101 context extraction router (Day 1 MVP)
app.include_router(ps101_router, prefix="/api/ps101")


class WimdRequest(BaseModel):
    prompt: str = Field(..., min_length=1)
    session_id: Optional[str] = None
    context: Optional[Dict[str, Any]] = None


class UserRegister(BaseModel):
    email: str = Field(..., min_length=1)
    password: str = Field(..., min_length=6)
    discount_code: Optional[str] = None


class UserLogin(BaseModel):
    email: str = Field(..., min_length=1)
    password: str = Field(..., min_length=1)


class UserResponse(BaseModel):
    user_id: str
    email: str
    created_at: str
    last_login: Optional[str] = None
    subscription_tier: Optional[str] = None


class ForceResetRequest(BaseModel):
    email: str = Field(..., min_length=1)
    new_password: str = Field(..., min_length=8)


class DiscountCodeValidate(BaseModel):
    code: str = Field(..., min_length=1, max_length=50)


class DiscountCodeResponse(BaseModel):
    valid: bool
    message: str
    grants_tier: Optional[str] = None


class WimdResponse(BaseModel):
    session_id: str
    message: str
    metrics: Dict[str, Any]


class UploadResponse(BaseModel):
    session_id: str
    file: Dict[str, Any]


class OpportunitiesResponse(BaseModel):
    session_id: str
    opportunities: List[Dict[str, Any]]


class ApplyRequest(BaseModel):
    job_id: str = Field(..., min_length=1)
    notes: Optional[str] = None


class ResumeRewriteRequest(BaseModel):
    session_id: Optional[str] = None
    job_id: Optional[str] = None
    version_name: Optional[str] = None
    source_resume: Optional[str] = None


class ResumeCustomizeRequest(BaseModel):
    session_id: Optional[str] = None
    job_id: str = Field(..., min_length=1)
    resume: str = Field(..., min_length=1)
    highlight_skills: Optional[List[str]] = None


class ResumeFeedbackRequest(BaseModel):
    session_id: Optional[str] = None
    resume: str = Field(..., min_length=1)


class ResumeVersionResponse(BaseModel):
    session_id: str
    versions: List[Dict[str, Any]]


def _clamp(value: float) -> int:
    return max(0, min(100, round(value)))


def _update_metrics(prompt: str, current: Dict[str, Any]) -> Dict[str, int]:
    words = len(re.findall(r"\w+", prompt))
    sentiment_boost = 4 if "thank" in prompt.lower() else 0
    clarity = _clamp(
        current.get("clarity", DEFAULT_METRICS["clarity"]) + min(words // 12, 6) + sentiment_boost
    )
    action = _clamp(current.get("action", DEFAULT_METRICS["action"]) + min(words // 18, 5))
    momentum = _clamp(current.get("momentum", DEFAULT_METRICS["momentum"]) + min(words // 20, 4))
    return {"clarity": clarity, "action": action, "momentum": momentum}


def _coach_reply(prompt: str, metrics: Dict[str, int], session_id: str = None) -> str:
    """Generate coach reply using PS101 flow or CSV→AI fallback system"""
    import json

    from .prompts_loader import read_registry

    # Check if PS101 is active for this session
    session_data = get_session_data(session_id) if session_id else {}

    # Auto-activate PS101 for new sessions (first message)
    is_first_message = not session_data or (
        not session_data.get("ps101_active") and not session_data.get("ps101_completed_at")
    )
    if is_first_message:
        # This is a new session or user hasn't started PS101 yet
        ps101_data = create_ps101_session_data()
        session_data.update(ps101_data)
        update_session_data(session_id, session_data)

        # Return first PS101 step prompt
        first_step = get_ps101_step(1)
        if first_step:
            return format_step_for_user(first_step)

    ps101_active = session_data.get("ps101_active", False)

    if ps101_active:
        # PS101 guided flow handling with conversational layer
        from api.conversational_coach import (
            detect_intent,
            generate_conversational_response,
            should_exit_ps101,
        )

        current_step = session_data.get("ps101_step", 1)
        current_prompt_idx = session_data.get("ps101_prompt_index", 0)

        # Build conversation history for context
        conversation_history = session_data.get("ps101_responses", [])

        # Get current question
        current_step_data = get_ps101_step(current_step)
        current_question = (
            current_step_data["prompts"][current_prompt_idx] if current_step_data else ""
        )

        # Detect user intent and tone
        intent, tone = detect_intent(prompt, current_question, conversation_history)

        # Check if user is responding to exit confirmation (must check FIRST)
        if session_data.get("ps101_exit_pending"):
            # User was asked to confirm exit, check response
            if "yes" in prompt.lower():
                # Confirmed exit
                session_data = exit_ps101_flow(session_data)
                session_data["ps101_exit_pending"] = False
                update_session_data(session_id, session_data)
                return "Understood. You can return to the guided process anytime by selecting 'Fast Track'. What would you like to explore next?"
            else:
                # User didn't confirm, clear flag and continue
                session_data["ps101_exit_pending"] = False
                update_session_data(session_id, session_data)
                # Fall through to conversational handling

        # Check for exit intent (more careful now)
        if should_exit_ps101(prompt, intent):
            # First exit attempt - ask for confirmation
            session_data["ps101_exit_pending"] = True
            update_session_data(session_id, session_data)
            return get_exit_confirmation()

        # Check if step is complete
        if ps101_is_complete(current_step):
            session_data = exit_ps101_flow(session_data)
            update_session_data(session_id, session_data)
            return get_completion_message()

        # Generate conversational response
        response, should_advance = generate_conversational_response(
            user_message=prompt,
            intent=intent,
            tone=tone,
            ps101_context=session_data,
            current_question=current_question,
            conversation_history=conversation_history,
            session_data=session_data,
        )

        # Record response if it's an answer
        from api.conversational_coach import UserIntent

        from .storage import get_user_id_for_session, record_ps101_db_response  # NEW IMPORTS

        if intent in [
            UserIntent.ANSWER,
            UserIntent.POSSIBILITY_THINKING,
            UserIntent.CIRCULAR_THINKING,
        ]:
            # Persist to database
            user_id_from_session = get_user_id_for_session(session_id)
            if user_id_from_session:
                record_ps101_db_response(
                    user_id=user_id_from_session,
                    step=current_step,
                    prompt_index=current_prompt_idx,
                    response=prompt,
                )
            else:
                logger.warning(
                    f"No user_id found for session {session_id}. PS101 response not persisted to DB."
                )

            # Continue updating session data (legacy behavior)
            session_data = record_ps101_response(session_data, current_step, prompt)

        # Advance if appropriate
        if should_advance:
            session_data = advance_ps101_step(session_data)

        update_session_data(session_id, session_data)

        # Return the conversational response (includes next question if advanced)
        return response

    # Normal CSV→AI fallback flow (PS101 not active)
    try:
        # PS101 COMPLETION GATE
        user_id = get_user_id_for_session(session_id)
        if user_id:
            ps101_context_data = get_user_context(user_id)
            if not ps101_context_data:
                # User has finished the flow, but context isn't extracted yet.
                # Or, they haven't finished the flow at all.
                return "Please complete the PS101 questionnaire first to get personalized coaching."
        else:
            # No user_id associated with the session, so no context is possible.
            return "It looks like you\'re not logged in. Please log in and complete the PS101 questionnaire for a personalized experience."

        # Get CSV prompts data
        csv_prompts = None
        try:
            reg = read_registry()
            active_sha = reg.get("active")
            if active_sha:
                for version in reg.get("versions", []):
                    if version["sha256"] == active_sha:
                        try:
                            with open(version["file"], encoding="utf-8") as f:
                                prompts_data = json.load(f)
                            csv_prompts = {"prompts": prompts_data}
                            break
                        except Exception:
                            continue
        except Exception:
            pass

        # Create a dynamic prompt with the context
        if ps101_context_data:
            system_prompt = f"""You are Mosaic, an expert career coach specializing in helping people design small, actionable experiments to test new career paths.

Your user has just completed the PS101 self-reflection exercise. This is their structured summary:
<ps101_context>
{json.dumps(ps101_context_data, indent=2)}
</ps101_context>

Your primary goal is to help them design their NEXT EXPERIMENT. Use their context—passions, skills, secret powers, and obstacles—to ask insightful questions and propose tiny, low-risk ways for them to test their assumptions.

- **DO NOT** mention \"PS101\" or the reflection process.
- **DO** use their \"key_quotes\" to build rapport and show you\'ve listened.
- **FOCUS ON ACTION.** Always be guiding towards a small, concrete next step.
- **Synthesize, don\'t just repeat.** Connect their passions and skills to potential experiments.
- **Challenge their obstacles.** Gently question their \"internal_obstacles\" and brainstorm ways around \"external_obstacles\".

Keep your responses concise, empathetic, and relentlessly focused on helping them build momentum through small wins.
"""
            context = {"metrics": metrics, "system_prompt": system_prompt}
        else:
            # This else block should ideally not be hit due to the completion gate above,
            # but it's here as a fallback.
            context = {"metrics": metrics}

        # Use prompt selector with CSV→AI fallback
        result = get_prompt_response(
            prompt=prompt,
            session_id=session_id or "default",
            csv_prompts=csv_prompts,
            context=context,
        )

        if result.get("response"):
            return result["response"]
        else:
            return _fallback_reply(metrics)

    except Exception:
        return _fallback_reply(metrics)


def _fallback_reply(metrics: Dict[str, int]) -> str:
    """Fallback reply when prompts can\'t be loaded"""
    return (
        "Noted. Here\'s where you sit right now — "
        f"clarity {metrics['clarity']}% · action {metrics['action']}% · "
        f"momentum {metrics['momentum']}%\. "
        "Pick one lever to nudge next."
    )


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Calculate cosine similarity between two vectors"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def get_embeddings(text: str) -> List[float]:
    """Get OpenAI embeddings for text"""
    try:
        settings = get_settings()
        if not settings.OPENAI_API_KEY:
            raise ValueError("OpenAI API key not configured")

        client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
        response = client.embeddings.create(model="text-embedding-3-small", input=text)
        return response.data[0].embedding
    except Exception as e:
        print(f"Error getting embeddings: {e}")
        return []


def semantic_search( 
    user_prompt: str, prompts_data: List[Dict], session_history: List[str] = None
) -> Optional[Dict]:
    """Find most semantically similar prompt using embeddings"""
    try:
        # Include session history for context
        context_text = user_prompt
        if session_history:
            context_text = f"{user_prompt} Context: {\' ".join(session_history[-3:])}"

        # Get user prompt embedding
        user_embedding = get_embeddings(context_text)
        if not user_embedding:
            return None

        best_match = None
        best_score = 0.0

        for prompt in prompts_data:
            # Compare against the prompt field (user's potential input), not completion
            prompt_text = prompt.get("prompt", "")
            if not prompt_text:
                continue

            # Get prompt embedding
            prompt_embedding = get_embeddings(prompt_text)
            if not prompt_embedding:
                continue

            # Calculate similarity
            similarity = cosine_similarity(user_embedding, prompt_embedding)

            if similarity > best_score:
                best_score = similarity
                best_match = prompt

        # Return best match if similarity is above threshold (lowered to 0.6 for better matching)
        return best_match if best_score > 0.6 else None

    except Exception as e:
        print(f"Error in semantic search: {e}")
        return None


def _score_job(metrics: Dict[str, int], job: Dict[str, Any]) -> Dict[str, Any]:
    base = (
        metrics.get("clarity", 0) * 0.4
        + metrics.get("action", 0) * 0.35
        + metrics.get("momentum", 0) * 0.25
    )
    skill_bias = 5 if "analysis" in job["skills"] and metrics.get("clarity", 0) > 70 else 0
    score = _clamp(base + skill_bias)
    return {
        "job_id": job["job_id"],
        "role": job["role"],
        "company": job["company"],
        "location": job["location"],
        "fit_score": score,
        "skills_match": job["skills"],
        "values_match": job["values"],
        "extras": {"source": "mosaic-generator"},
    }

def _generate_matches(metrics: Dict[str, int]) -> List[Dict[str, Any]]:
    matches = [_score_job(metrics, job) for job in JOB_LIBRARY]
    return sorted(matches, key=lambda item: item["fit_score"], reverse=True)

def _rewrite_resume_text(
    metrics: Dict[str, int], job_id: Optional[str], source: Optional[str]
) -> str:
    summary = (
        f"Clarity {metrics.get('clarity', 0)} · Action {metrics.get('action', 0)} · "
        f"Momentum {metrics.get('momentum', 0)}"
    )
    header = "Resume Draft — Mosaic"
    target = f"Target Role: {job_id}" if job_id else "Exploratory Draft"
    body = (
        source.strip()
        if source
        else "• Translate delta analysis into narrative impact\n• Highlight fast-track wins"
    )
    return f"{header}\n{target}\nMetrics: {summary}\n\n{body}\n"

def _resolve_session(
    request_session: Optional[str],
    header_session: Optional[str],
    *, 
    allow_create: bool,
) -> str:
    candidate = request_session or header_session
    if allow_create:
        return ensure_session(candidate)
    if not candidate or not session_exists(candidate):
        raise HTTPException(status_code=404, detail="session_not_found")
    ensure_session(candidate)
    return candidate


@app.on_event("startup")
async def _startup():
    await startup_or_die()

    # Clear prompt cache on startup (cache was returning placeholder "Cached response")
    try:
        from .storage import get_conn

        with get_conn() as conn:
            cursor = conn.cursor()  # OK: PostgreSQL pattern
            cursor.execute("DELETE FROM prompt_selector_cache")
            print("✓ Cleared prompt_selector_cache on startup")
    except Exception as e:
        print(f"⚠️ Failed to clear cache on startup: {e}")

    SERVICE_READY.set()


@app.get("/")
def root():
    s = get_settings()
    return {
        "message": "Mosaic Platform API - Complete Implementation",
        "interface_design": "Integrated with minimal app architecture",
        "deployment_timestamp": datetime.utcnow().isoformat() + "Z",
        "cache_bust": "nuclear_reset_complete",
        "endpoints": {
            "health": "/health",
            "config": "/config",
            "prompts": "/prompts/active",
            "wimd": "/wimd",
            "wimd_upload": "/wimd/upload",
            "wimd_metrics": "/wimd/metrics",
            "wimd_analysis": "/wimd/analysis",
            "ob_opportunities": "/ob/opportunities",
            "ob_matches": "/ob/matches",
            "ob_status": "/ob/status",
            "ob_apply": "/ob/apply",
            "resume_rewrite": "/resume/rewrite",
            "resume_customize": "/resume/customize",
            "resume_feedback": "/resume/feedback",
            "resume_versions": "/resume/versions",
        },
        "schemaVersion": s.APP_SCHEMA_VERSION,
    }


@app.get("/health")
def health():
    """Enhanced health check with prompt system monitoring for auto-restart"""
    try:
        if not SERVICE_READY.is_set():
            status = {
                "ok": True,
                "status": "initializing",
                "checks": {
                    "startup_complete": False,
                },
                "timestamp": datetime.utcnow().isoformat() + "Z",
            }
            if HEALTH_DEBUG_ENABLED:
                logger.info("Health check in startup grace period: %s", status)
            return status

        # Test critical prompt system functionality
        from .prompt_selector import get_prompt_health

        prompt_health = get_prompt_health()

        # Check if prompt system is working
        fallback_enabled = prompt_health.get("fallback_enabled", False)
        ai_available = prompt_health.get("ai_health", {}).get("any_available", False)

        if HEALTH_DEBUG_ENABLED:
            logger.info(
                "HEALTH CHECK DEBUG fallback_enabled=%s (type=%s) ai_available=%s prompt_health=%s",
                fallback_enabled,
                type(fallback_enabled),
                ai_available,
                prompt_health,
            )

        # System is healthy if either CSV works OR AI fallback is available
        prompt_system_ok = fallback_enabled or ai_available

        # Check database connectivity
        db_ok = True
        try:
            with get_conn() as conn:
                cursor = conn.cursor()  # OK: PostgreSQL pattern
                cursor.execute("SELECT 1")
                cursor.fetchone()
        except Exception as e:
            logger.error("Database connectivity check failed: %s", e, exc_info=True)
            db_ok = False

        # Overall health
        overall_ok = prompt_system_ok and db_ok

        health_status = {
            "ok": overall_ok,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "checks": {
                "database": db_ok,
                "prompt_system": prompt_system_ok,
                "ai_fallback_enabled": fallback_enabled,
                "ai_available": ai_available,
            },
        }

        if HEALTH_DEBUG_ENABLED:
            logger.info(
                "Health check passed"
            )
        return health_status

    except HTTPException:
        raise
    except Exception as e:
        logger.error("Health check raised unexpected exception", exc_info=True)
        # Critical failure - return 503 to trigger restart
        raise HTTPException(
            status_code=503,
            detail={"ok": False, "error": str(e), "timestamp": datetime.utcnow().isoformat() + "Z"},
        )


@app.get("/health/prompts")
def health_prompts():
    """Health check for prompt selector and AI fallback system"""
    try:
        prompt_health = get_prompt_health()
        return {
            "ok": True,
            "prompt_selector": prompt_health,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
    except Exception as e:
        return {"ok": False, "error": str(e), "timestamp": datetime.utcnow().isoformat() + "Z"}


@app.get("/health/booking")
def health_booking():
    """Health check for booking router"""
    return {
        "booking_router_loaded": BOOKING_ROUTER_ERROR is None,
        "error": BOOKING_ROUTER_ERROR,
        "timestamp": datetime.utcnow().isoformat() + "Z",
    }


@app.get("/health/comprehensive")
def health_comprehensive():
    """Comprehensive health check with automatic recovery"""
    try:
        health_summary = run_health_check()

        # If system needs attention, log it
        if health_summary.get("requires_attention", False):
            print(f"⚠️ Prompt system requires attention: {health_summary}")

        # Return 503 if critical failure to trigger Render restart
        if not health_summary.get("current_test", {}).get("success", False):
            raise HTTPException(status_code=503, detail=health_summary)

        return health_summary
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=503,
            detail={"ok": False, "error": str(e), "timestamp": datetime.utcnow().isoformat() + "Z"},
        )


@app.post("/health/recover")
def health_recover():
    """Attempt automatic system recovery"""
    try:
        recovery_result = attempt_system_recovery()
        return recovery_result
    except Exception as e:
        return {
            "recovery_attempted": False,
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }


@app.get("/health/experiments")
def health_experiments():
    """Health check for experiment engine"""
    try:
        experiment_health = get_experiment_health()
        return {
            "ok": True,
            "experiment_engine": experiment_health,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
    except Exception as e:
        return {"ok": False, "error": str(e), "timestamp": datetime.utcnow().isoformat() + "Z"}


@app.get("/config")
def config():
    s = get_settings()
    return {"apiBase": os.getenv("PUBLIC_API_BASE", ""), "schemaVersion": s.APP_SCHEMA_VERSION}


@app.get("/prompts/active")
def prompts_active():
    from .prompts_loader import read_registry

    reg = read_registry()
    return {"active": reg.get("active")}


@app.get("/prompts/{sha}")
def get_prompts(sha: str):
    """Get prompts content by SHA"""
    import json

    from .prompts_loader import read_registry

    reg = read_registry()
    if not reg.get("active"):
        raise HTTPException(status_code=404, detail="No active prompts")

    # Find the version with this SHA
    for version in reg.get("versions", []):
        if version["sha256"] == sha:
            try:
                with open(version["file"], encoding="utf-8") as f:
                    prompts = json.load(f)
                return {"sha": sha, "prompts": prompts}
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Error loading prompts: {e!s}")

    raise HTTPException(status_code=404, detail="Prompts not found")


@app.get("/debug/cors")
def debug_cors():
    """Debug endpoint to show configured CORS origins"""
    return {
        "cors_origins": cors_origins,
        "public_site_origin_env": os.getenv("PUBLIC_SITE_ORIGIN", "NOT SET"),
        "middleware_config": {
            "allow_credentials": True,
            "allow_methods": ["GET", "POST", "OPTIONS"],
            "allow_headers": ["content-type", "authorization", "x-session-id"],
        },
    }


@app.options("/wimd/upload")
def wimd_upload_options():
    """Explicit OPTIONS handler for Render edge compatibility"""
    return Response(status_code=200)


@app.post("/wimd", response_model=WimdResponse)
async def wimd_chat(
    payload: WimdRequest,
    session_header: Optional[str] = Header(None, alias="X-Session-ID"),
):
    session_id = _resolve_session(payload.session_id, session_header, allow_create=True)
    current_metrics = latest_metrics(session_id) or DEFAULT_METRICS
    metrics = _update_metrics(payload.prompt, current_metrics)
    message = _coach_reply(payload.prompt, metrics, session_id)
    record_wimd_output(
        session_id,
        payload.prompt,
        message,
        analysis_data={"context": payload.context or {}},
        metrics=metrics,
    )
    return WimdResponse(session_id=session_id, message=message, metrics=metrics)


@app.get("/wimd/metrics")
def wimd_metrics(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    session_id = _resolve_session(None, session_header, allow_create=False)
    metrics = latest_metrics(session_id)
    if not metrics:
        raise HTTPException(status_code=404, detail="metrics_not_found")
    return {"session_id": session_id, "metrics": metrics}


@app.get("/wimd/analysis")
def wimd_analysis(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    session_id = _resolve_session(None, session_header, allow_create=False)
    history = wimd_history(session_id)
    return {"session_id": session_id, "history": history}


@app.post("/wimd/upload", response_model=UploadResponse)
async def wimd_upload(
    background: BackgroundTasks,
    file: UploadFile = File(...),
    session_header: Optional[str] = Header(None, alias="X-Session-ID"),
):
    session_id = _resolve_session(None, session_header, allow_create=True)
    if not file.filename:
        raise HTTPException(status_code=400, detail="filename_required")
    meta = await _save_upload(session_id, file)
    background.add_task(file.close)
    metrics = latest_metrics(session_id) or DEFAULT_METRICS
    return UploadResponse(session_id=session_id, file={**meta, "metrics": metrics})


@app.get("/ob/opportunities", response_model=OpportunitiesResponse)
def ob_opportunities(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    session_id = _resolve_session(None, session_header, allow_create=False)
    metrics = latest_metrics(session_id)
    if not metrics:
        raise HTTPException(status_code=404, detail="metrics_not_ready")
    matches = _generate_matches(metrics)
    store_job_matches(session_id, matches)
    return OpportunitiesResponse(session_id=session_id, opportunities=matches)


@app.get("/ob/matches")
def ob_matches(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    session_id = _resolve_session(None, session_header, allow_create=False)
    return {"session_id": session_id, "matches": fetch_job_matches(session_id)}


@app.get("/ob/status")
def ob_status(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    session_id = _resolve_session(None, session_header, allow_create=False)
    matches = fetch_job_matches(session_id)
    applied = [match for match in matches if match.get("extras", {}).get("status") == "applied"]
    return {
        "session_id": session_id,
        "applied": applied,
        "available": matches,
    }


@app.options("/ob/apply")
def ob_apply_options():
    """Explicit OPTIONS handler for Render edge compatibility"""
    return Response(status_code=200)


@app.post("/ob/apply")
def ob_apply(
    payload: ApplyRequest,
    session_header: Optional[str] = Header(None, alias="X-Session-ID"),
):
    session_id = _resolve_session(None, session_header, allow_create=False)
    matches = {match["job_id"]: match for match in fetch_job_matches(session_id)}
    if payload.job_id not in matches:
        raise HTTPException(status_code=404, detail="job_match_not_found")
    try:
        update_job_match_status(session_id, payload.job_id, status="applied", notes=payload.notes)
    except ValueError:
        raise HTTPException(status_code=404, detail="job_match_not_found")
    return {
        "session_id": session_id,
        "job": matches[payload.job_id],
        "status": "applied",
        "notes": payload.notes or "",
    }


@app.options("/resume/rewrite")
def resume_rewrite_options():
    """Explicit OPTIONS handler for Render edge compatibility"""
    return Response(status_code=200)


@app.post("/resume/rewrite")
def resume_rewrite(
    payload: ResumeRewriteRequest,
    session_header: Optional[str] = Header(None, alias="X-Session-ID"),
):
    session_id = _resolve_session(payload.session_id, session_header, allow_create=True)
    metrics = latest_metrics(session_id) or DEFAULT_METRICS
    draft = _rewrite_resume_text(metrics, payload.job_id, payload.source_resume)
    version_name = payload.version_name or f"canonical-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
    version_id = add_resume_version(session_id, version_name, draft, payload.job_id)
    return {
        "session_id": session_id,
        "version_id": version_id,
        "version_name": version_name,
        "resume": draft,
    }


@app.options("/resume/customize")
def resume_customize_options():
    """Explicit OPTIONS handler for Render edge compatibility"""
    return Response(status_code=200)


@app.post("/resume/customize")
def resume_customize(
    payload: ResumeCustomizeRequest,
    session_header: Optional[str] = Header(None, alias="X-Session-ID"),
):
    session_id = _resolve_session(payload.session_id, session_header, allow_create=False)
    metrics = latest_metrics(session_id) or DEFAULT_METRICS
    highlights = payload.highlight_skills or []
    suffix = (
        "\n\nHighlighted Skills:\n" + "\n".join(f"• {item}" for item in highlights)
        if highlights
        else ""
    )
    draft = _rewrite_resume_text(metrics, payload.job_id, payload.resume) + suffix
    version_name = f"custom-{payload.job_id}-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
    version_id = add_resume_version(session_id, version_name, draft, payload.job_id)
    return {
        "session_id": session_id,
        "version_id": version_id,
        "version_name": version_name,
        "resume": draft,
    }


@app.options("/resume/feedback")
def resume_feedback_options():
    """Explicit OPTIONS handler for Render edge compatibility"""
    return Response(status_code=200)


@app.post("/resume/feedback")
def resume_feedback(
    payload: ResumeFeedbackRequest,
    session_header: Optional[str] = Header(None, alias="X-Session-ID"),
):
    session_id = _resolve_session(payload.session_id, session_header, allow_create=False)
    line_count = payload.resume.count("\n") + 1
    word_count = len(re.findall(r"\w+", payload.resume))
    suggestions = []
    if line_count < 20:
        suggestions.append("Add more evidence-based bullets to demonstrate scope.")
    if "impact" not in payload.resume.lower():
        suggestions.append("Surface measurable impact (e.g., % lift, revenue, time saved).")
    if "summary" not in payload.resume.lower():
        suggestions.append("Consider opening with a 2-3 sentence summary that anchors your delta.")
    add_resume_version(
        session_id,
        f"feedback-{datetime.utcnow().strftime('%H%M%S')}",
        payload.resume,
        feedback={"suggestions": suggestions},
    )
    return {
        "session_id": session_id,
        "analysis": {"word_count": word_count, "line_count": line_count},
        "suggestions": suggestions,
    }


# Discount Code Endpoints
@app.post("/auth/validate-code", response_model=DiscountCodeResponse)
def validate_discount_code(payload: DiscountCodeValidate):
    """Validate a discount code"""
    code = payload.code.strip().upper()

    with get_conn() as conn:
        cursor = conn.cursor()
        cursor.execute(
            """
            SELECT code, grants_tier, max_uses, current_uses, active, expires_at
            FROM discount_codes
            WHERE UPPER(code) = ?
        """,
            (code,),
        )
        result = cursor.fetchone()

    if not result:
        return DiscountCodeResponse(valid=False, message="Invalid discount code")

    code_value, grants_tier, max_uses, current_uses, active, expires_at = result

    # Check if active
    if not active:
        return DiscountCodeResponse(valid=False, message="This code is no longer active")

    # Check expiration
    from datetime import datetime

    if expires_at and datetime.now() > expires_at:
        return DiscountCodeResponse(valid=False, message="This code has expired")

    # Check usage limit
    if max_uses is not None and current_uses >= max_uses:
        return DiscountCodeResponse(valid=False, message="This code has reached its usage limit")

    return DiscountCodeResponse(
        valid=True, message=f"Code valid - grants {grants_tier} access", grants_tier=grants_tier
    )


# User Authentication Endpoints
@app.post("/auth/register", response_model=UserResponse)
async def register_user(payload: UserRegister):
    """Register a new user with optional discount code"""
    payload.email = payload.email.strip()
    # Check if user already exists
    existing_user = get_user_by_email(payload.email)
    if existing_user:
        raise HTTPException(status_code=400, detail="User already exists")

    subscription_tier = "free"
    subscription_status = "active"
    discount_code_used = None

    # If discount code provided, validate and consume it
    if payload.discount_code:
        code = payload.discount_code.strip().upper()

        with get_conn() as conn:
            cursor = conn.cursor()

            # Validate code
            cursor.execute(
                """
                SELECT code, grants_tier, max_uses, current_uses, active, expires_at
                FROM discount_codes
                WHERE UPPER(code) = ?
            """,
                (code,),
            )
            code_result = cursor.fetchone()

            if not code_result:
                raise HTTPException(status_code=400, detail="Invalid discount code")

            code_value, grants_tier, max_uses, current_uses, active, expires_at = code_result

            if not active:
                raise HTTPException(status_code=400, detail="Code is no longer active")

            from datetime import datetime

            if expires_at and datetime.now() > expires_at:
                raise HTTPException(status_code=400, detail="Code has expired")

            if max_uses is not None and current_uses >= max_uses:
                raise HTTPException(status_code=400, detail="Code usage limit reached")

            # Grant tier from code
            subscription_tier = grants_tier
            discount_code_used = code_value

            # Increment usage
            cursor.execute(
                """
                UPDATE discount_codes
                SET current_uses = current_uses + 1
                WHERE code = ?
            """,
                (code_value,),
            )
            conn.commit()

    # Create new user with subscription info
    user_id = create_user(
        payload.email, payload.password, subscription_tier, subscription_status, discount_code_used
    )
    user = get_user_by_id(user_id)

    return UserResponse(
        user_id=user["user_id"],
        email=user["email"],
        created_at=user["created_at"],
        last_login=user["last_login"],
        subscription_tier=user.get("subscription_tier"),
        subscription_status=user.get("subscription_status"),
    )


@app.post("/auth/login", response_model=UserResponse)
async def login_user(payload: UserLogin):
    """Login user and return user data"""
    payload.email = payload.email.strip()
    user_id = authenticate_user(payload.email, payload.password)
    if not user_id:
        raise HTTPException(status_code=401, detail="Invalid credentials")

    user = get_user_by_id(user_id)
    return UserResponse(
        user_id=user["user_id"],
        email=user["email"],
        created_at=user["created_at"],
        last_login=user["last_login"],
    )


@app.get("/auth/me", response_model=UserResponse)
async def get_current_user(user_id: str = Header(..., alias="X-User-ID")):
    """Get current user data"""
    user = get_user_by_id(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    return UserResponse(
        user_id=user["user_id"],
        email=user["email"],
        created_at=user["created_at"],
        last_login=user["last_login"],
    )


@app.post("/auth/reset-password")
async def reset_password(email: str = Body(..., embed=True)):
    """Send password reset email (placeholder - needs email service)"""
    normalized_email = (email or "").strip().lower()
    try:
        with get_conn() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT id FROM users WHERE LOWER(email) = LOWER(?)", (normalized_email,)
            )
            user = cursor.fetchone()
    except Exception as exc:
        print(f"[AUTH] reset_password lookup failed: {exc}")
        user = None

    # Do not reveal whether the account exists; success either way
    return {"message": "If that email exists, a reset link has been sent"}


@app.get("/auth/diagnose/{email}")
async def diagnose_user_password_hash(
    email: str,
    admin_key: str = Header(None, alias="X-Admin-Key"),
):
    """Diagnose password hash format for a user (admin debug only)"""
    expected_key = os.getenv("ADMIN_DEBUG_KEY")

    if not expected_key:
        raise HTTPException(status_code=503, detail="Admin debug not configured")

    if not admin_key or admin_key != expected_key:
        raise HTTPException(status_code=403, detail="Unauthorized")

    result = diagnose_user_hash(email)
    if not result:
        raise HTTPException(status_code=404, detail="User not found")

    return result


@app.post("/auth/force-reset")
async def force_reset_password(
    payload: ForceResetRequest,
    admin_key: str = Header(None, alias="X-Admin-Key"),
):
    """Force reset user password (admin debug only)"""
    expected_key = os.getenv("ADMIN_DEBUG_KEY")

    if not expected_key:
        raise HTTPException(status_code=503, detail="Admin debug not configured")

    if not admin_key or admin_key != expected_key:
        raise HTTPException(status_code=403, detail="Unauthorized")

    success = force_reset_user_password(payload.email, payload.new_password)
    if not success:
        raise HTTPException(status_code=404, detail="User not found")

    return {"success": True, "message": f"Password reset for {payload.email}"}


@app.post("/auth/logout")
async def logout_user(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    """Logout user and delete session from database"""
    if not session_header:
        raise HTTPException(status_code=400, detail="No session to logout")

    # Delete the session and all related data
    try:
        delete_session(session_header)
        return {"message": "Logged out successfully"}
    except Exception:
        # Even if deletion fails, return success (session might not exist)
        return {"message": "Logged out successfully"}


@app.get("/resume/versions", response_model=ResumeVersionResponse)
def resume_versions(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    session_id = _resolve_session(None, session_header, allow_create=False)
    versions = list_resume_versions(session_id)
    return ResumeVersionResponse(session_id=session_id, versions=versions)


@app.get("/session/summary")
def session_summary_endpoint(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    session_id = _resolve_session(None, session_header, allow_create=False)
    return session_summary(session_id)


# Experiment Engine Endpoints
@app.post("/experiments/create")
def experiments_create(
    experiment_data: ExperimentCreate,
    session_header: Optional[str] = Header(None, alias="X-Session-ID"),
):
    """Create a new experiment"""
    session_id = _resolve_session(None, session_header, allow_create=True)
    return create_experiment(session_id, experiment_data)


@app.post("/experiments/update")
def experiments_update(
    experiment_data: ExperimentUpdate,
    session_header: Optional[str] = Header(None, alias="X-Session-ID"),
):
    """Update an existing experiment"""
    session_id = _resolve_session(None, session_header, allow_create=False)
    return update_experiment(session_id, experiment_data)


@app.post("/experiments/complete")
def experiments_complete(
    experiment_id: str, session_header: Optional[str] = Header(None, alias="X-Session-ID")
):
    """Mark an experiment as completed"""
    session_id = _resolve_session(None, session_header, allow_create=False)
    return complete_experiment(session_id, experiment_id)


@app.post("/learning/add")
def learning_add(
    learning_data: LearningData,
    session_header: Optional[str] = Header(None, alias="X-Session-ID")
):
    """Add learning data to an experiment"""
    session_id = _resolve_session(None, session_header, allow_create=False)
    return add_learning_data(session_id, learning_data)


@app.post("/evidence/capture")
def evidence_capture(
    evidence_data: CapabilityEvidence,
    session_header: Optional[str] = Header(None, alias="X-Session-ID"),
):
    """Capture capability evidence"""
    session_id = _resolve_session(None, session_header, allow_create=True)
    return capture_evidence(session_id, evidence_data)


@app.post("/metrics/self-efficacy")
def metrics_self_efficacy(
    metric_data: SelfEfficacyMetric,
    session_header: Optional[str] = Header(None, alias="X-Session-ID")
):
    """Record a self-efficacy metric"""
    session_id = _resolve_session(None, session_header, allow_create=True)
    return record_self_efficacy_metric(session_id, metric_data)


@app.get("/experiments")
def experiments_list(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    """Get all experiments for a session"""
    session_id = _resolve_session(None, session_header, allow_create=False)
    experiments = get_experiments(session_id)
    return {"session_id": session_id, "experiments": experiments}


@app.get("/learning")
def learning_list(
    experiment_id: Optional[str] = None,
    session_header: Optional[str] = Header(None, alias="X-Session-ID")
):
    """Get learning data for a session or specific experiment"""
    session_id = _resolve_session(None, session_header, allow_create=False)
    learning_data = get_learning_data(session_id, experiment_id)
    return {"session_id": session_id, "learning_data": learning_data}


@app.get("/metrics")
def metrics_list(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    """Get self-efficacy metrics for a session"""
    session_id = _resolve_session(None, session_header, allow_create=False)
    metrics = get_self_efficacy_metrics(session_id)
    return {"session_id": session_id, "metrics": metrics}


# Self-Efficacy Engine Endpoints
@app.get("/self-efficacy/metrics")
def self_efficacy_metrics(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    """Get computed self-efficacy metrics for a session"""
    session_id = _resolve_session(None, session_header, allow_create=False)
    metrics = compute_session_metrics(session_id)

    # Record analytics entry
    record_analytics_entry(session_id, metrics)

    return {
        "session_id": session_id,
        "experiment_completion_rate": metrics.experiment_completion_rate,
        "learning_velocity": metrics.learning_velocity,
        "confidence_score": metrics.confidence_score,
        "escalation_risk": metrics.escalation_risk,
        "total_experiments": metrics.total_experiments,
        "completed_experiments": metrics.completed_experiments,
        "learning_events": metrics.learning_events,
        "days_active": metrics.days_active,
        "last_activity": metrics.last_activity,
        "metrics_timestamp": metrics.metrics_timestamp,
    }


@app.get("/self-efficacy/escalation")
def self_efficacy_escalation(session_header: Optional[str] = Header(None, alias="X-Session-ID")):
    """Check if session should be escalated to human coach"""
    session_id = _resolve_session(None, session_header, allow_create=False)
    should_esc, reason = should_escalate(session_id)
    prompt = get_escalation_prompt(session_id) if should_esc else None

    return {
        "session_id": session_id,
        "should_escalate": should_esc,
        "reason": reason,
        "escalation_prompt": prompt,
    }


@app.post("/self-efficacy/cleanup")
def self_efficacy_cleanup(days_threshold: int = 30):
    """Clean up stale experiments (admin endpoint)"""
    result = cleanup_stale_experiments(days_threshold)
    return result


@app.get("/health/self-efficacy")
def health_self_efficacy():
    """Health check for self-efficacy engine"""
    try:
        health = get_self_efficacy_health()
        return {"ok": True, "self_efficacy_engine": health, "timestamp": datetime.utcnow().isoformat() + "Z"}
    except Exception as e:
        return {"ok": False, "error": str(e), "timestamp": datetime.utcnow().isoformat() + "Z"}


# RAG Engine Endpoints
@app.get("/rag/embed")
def rag_embed(text: str):
    """Compute embedding for text"""
    try:
        result = compute_embedding(text)
        if result:
            return {
                "text": result.text,
                "embedding": result.embedding,
                "model": result.model,
                "cached": result.cached,
                "created_at": result.created_at,
            }
        else:
            return {"error": "Failed to compute embedding"}
    except Exception as e:
        return {"error": str(e)}


@app.get("/rag/batch-embed")
def rag_batch_embed(texts: str):  # Comma-separated texts
    """Batch compute embeddings for multiple texts"""
    try:
        text_list = [t.strip() for t in texts.split(",")]
        results = batch_compute_embeddings(text_list)
        return {
            "texts": text_list,
            "embeddings": [
                {
                    "text": result.text,
                    "embedding": result.embedding,
                    "model": result.model,
                    "cached": result.cached,
                    "created_at": result.created_at,
                }
                for result in results
            ],
            "total_processed": len(results),
        }
    except Exception as e:
        return {"error": str(e)}


@app.get("/rag/retrieve")
def rag_retrieve(query: str, limit: int = 5, min_similarity: float = 0.7):
    """Retrieve similar content using RAG"""
    try:
        result = retrieve_similar(query, limit, min_similarity)
        return {
            "query": result.query,
            "matches": result.matches,
            "confidence": result.confidence,
            "fallback_used": result.fallback_used,
            "retrieval_time": result.retrieval_time,
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/rag/query")
def rag_query(query: str, context: Dict[str, Any] = None):
    """Get RAG response with retrieval and fallback"""
    try:
        result = get_rag_response(query, context)
        return result
    except Exception as e:
        return {"error": str(e)}


@app.post("/rag/domain-adjacent")
def rag_domain_adjacent(request: dict):
    """Discover domain adjacent opportunities using RAG semantic clustering."""
    try:
        user_skills = request.get("user_skills", [])
        user_domains = request.get("user_domains", [])

        # Use RAG-powered domain adjacent search
        results = discover_domain_adjacent_opportunities_rag(user_skills, user_domains)

        return results
    except Exception as e:
        return {"error": str(e)}


@app.get("/health/rag")
def health_rag():
    """Health check for RAG engine"""
    try:
        health = get_rag_health()
        return {"ok": True, "rag_engine": health, "timestamp": datetime.utcnow().isoformat() + "Z"}
    except Exception as e:
        return {"ok": False, "error": str(e), "timestamp": datetime.utcnow().isoformat() + "Z"}


# Job Sources Endpoints
@app.get("/jobs/search")
def jobs_search(query: str, location: str = None, limit: int = 10):
    """Search jobs across all sources"""
    try:
        # Check cost limits first
        cost_check = check_cost_limits("job_search", 0.01)  # $0.01 per job search
        if not cost_check["allowed"]:
            return {"error": f"Cost limit exceeded: {cost_check['reason']}", "cost_limit": True}

        # Check resource limits
        resource_check = check_resource_limits("job_search")
        if not resource_check["allowed"]:
            return {
                "error": f"Resource limit exceeded: {resource_check['reason']}",
                "resource_limit": True,
            }

        # Initialize job sources
        greenhouse = GreenhouseSource()
        serpapi = SerpApiSource()
        reddit = RedditSource()

        all_jobs = []
        success_count = 0

        # Search each source
        for source in [greenhouse, serpapi, reddit]:
            try:
                jobs = source.search_jobs(query, location, limit)
                all_jobs.extend(jobs)
                success_count += 1
            except Exception as e:
                print(f"Error searching {source.name}: {e}")
                continue

        # Remove duplicates and limit results
        unique_jobs = []
        seen_ids = set()
        for job in all_jobs:
            if job.id not in seen_ids:
                unique_jobs.append(job)
                seen_ids.add(job.id)

        # Record usage
        record_usage("job_search", 0.01, success_count > 0)

        return {
            "query": query,
            "location": location,
            "total_results": len(unique_jobs),
            "sources_used": success_count,
            "jobs": [
                {
                    "id": job.id,
                    "title": job.title,
                    "company": job.company,
                    "location": job.location,
                    "description": (
                        job.description[:200] + "..."
                        if len(job.description) > 200
                        else job.description
                    ),
                    "url": job.url,
                    "source": job.source,
                    "remote": job.remote,
                    "skills": job.skills,
                    "experience_level": job.experience_level,
                }
                for job in unique_jobs[:limit]
            ],
        }
    except Exception as e:
        record_usage("job_search", 0.01, False)
        return {"error": str(e)}


@app.get("/jobs/search/rag")
def jobs_search_rag(query: str, location: str = None, limit: int = 10):
    """RAG-powered job search with dynamic source discovery"""
    try:
        # Use RAG to discover optimal sources
        optimal_sources = get_optimal_sources_for_query(query, location)

        # Initialize all available sources (production-ready only by default)
        source_map = {
            # Production-ready sources (no API key required)
            "greenhouse": GreenhouseSource(),
            "serpapi": SerpApiSource(),
            "reddit": RedditSource(),
            "remoteok": RemoteOKSource(),
            # "weworkremotely": WeWorkRemotelySource(),
            "hackernews": HackerNewsSource(),
        }

        # Add stubbed sources only if feature flag is enabled
        if get_feature_flag("JOB_SOURCES_STUBBED_ENABLED"):
            source_map.update(
                {
                    # "indeed": IndeedSource(),
                    "linkedin": LinkedInSource(),
                    "glassdoor": GlassdoorSource(),
                    "dice": DiceSource(),
                    "monster": MonsterSource(),
                    "ziprecruiter": ZipRecruiterSource(),
                    "careerbuilder": CareerBuilderSource(),
                }
            )

        all_jobs = []
        used_sources = []

        # Search optimal sources first
        for source_name in optimal_sources:
            if source_name in source_map:
                try:
                    source = source_map[source_name]
                    jobs = source.search_jobs(query, location, limit)
                    all_jobs.extend(jobs)
                    used_sources.append(source_name)
                except Exception as e:
                    print(f"Error searching {source_name}: {e}")
                    continue

        # Fallback to other sources if needed
        if len(all_jobs) < limit:
            for source_name, source in source_map.items():
                if source_name not in used_sources:
                    try:
                        jobs = source.search_jobs(query, location, limit)
                        all_jobs.extend(jobs)
                        used_sources.append(source_name)
                    except Exception as e:
                        print(f"Error searching {source_name}: {e}")
                        continue

        # Remove duplicates and limit results
        unique_jobs = []
        seen_ids = set()
        for job in all_jobs:
            if job.id not in seen_ids:
                unique_jobs.append(job)
                seen_ids.add(job.id)

        return {
            "query": query,
            "location": location,
            "rag_optimized": True,
            "optimal_sources": optimal_sources,
            "used_sources": used_sources,
            "total_results": len(unique_jobs),
            "jobs": [
                {
                    "id": job.id,
                    "title": job.title,
                    "company": job.company,
                    "location": job.location,
                    "description": (
                        job.description[:200] + "..."
                        if len(job.description) > 200
                        else job.description
                    ),
                    "url": job.url,
                    "source": job.source,
                    "remote": job.remote,
                    "skills": job.skills,
                    "experience_level": job.experience_level,
                }
                for job in unique_jobs[:limit]
            ],
        }
    except Exception as e:
        return {"error": str(e)}


@app.get("/jobs/{job_id}")
def get_job_details(job_id: str):
    """Get detailed job information"""
    try:
        # Try each source to find the job
        sources = [GreenhouseSource(), SerpApiSource(), RedditSource()]

        for source in sources:
            try:
                job = source.get_job_details(job_id)
                if job:
                    return {
                        "id": job.id,
                        "title": job.title,
                        "company": job.company,
                        "location": job.location,
                        "description": job.description,
                        "url": job.url,
                        "source": job.source,
                        "posted_date": job.posted_date.isoformat() if job.posted_date else None,
                        "salary_range": job.salary_range,
                        "job_type": job.job_type,
                        "remote": job.remote,
                        "skills": job.skills,
                        "experience_level": job.experience_level,
                        "metadata": job.metadata,
                    }
            except Exception as e:
                print(f"Error getting job details from {source.name}: {e}")
                continue

        return {"error": "Job not found"}
    except Exception as e:
        return {"error": str(e)}


# RAG Source Discovery Endpoints
@app.get("/sources/discover")
def discover_sources(query: str, location: str = None, job_type: str = None):
    """Discover optimal sources for a job search query using RAG"""
    try:
        discoveries = discover_sources_for_query(query, location, job_type)
        return {
            "query": query,
            "location": location,
            "job_type": job_type,
            "discoveries": [
                {
                    "source_name": discovery.source_name,
                    "source_type": discovery.source_type,
                    "api_endpoint": discovery.api_endpoint,
                    "rate_limit": discovery.rate_limit,
                    "confidence": discovery.confidence,
                    "discovery_reason": discovery.discovery_reason,
                    "integration_status": discovery.integration_status,
                }
                for discovery in discoveries
            ],
        }
    except Exception as e:
        return {"error": str(e)}


@app.get("/sources/analytics")
def get_source_analytics():
    """Get analytics on source discovery and integration"""
    try:
        analytics = get_discovery_analytics()
        return analytics
    except Exception as e:
        return {"error": str(e)}


# Cost Control Endpoints
@app.get("/cost/analytics")
def get_cost_analytics():
    """Get cost and usage analytics"""
    try:
        analytics = get_usage_analytics()
        return analytics
    except Exception as e:
        return {"error": str(e)}


@app.get("/cost/limits")
def get_cost_limits():
    """Get current cost limits and usage"""
    try:
        analytics = get_usage_analytics()
        return {
            "cost_limits": analytics.get("cost_limits", {}),
            "resource_limits": analytics.get("resource_limits", {}),
            "current_usage": analytics.get("current_usage", {}),
            "emergency_stop": analytics.get("emergency_stop", False),
            "status": analytics.get("status", "unknown"),
        }
    except Exception as e:
        return {"error": str(e)}


# Competitive Intelligence Endpoints
@app.get("/intelligence/company/{company_name}")
def analyze_company(company_name: str, industry: str = None):
    """Analyze company strategic needs and pain points."""
    try:
        analysis = analyze_company_strategic_needs(company_name, industry)
        return {
            "company_name": analysis.company_name,
            "industry": analysis.industry,
            "size": analysis.size,
            "pain_points": analysis.pain_points,
            "key_priorities": analysis.key_priorities,
            "competitive_advantages": analysis.competitive_advantages,
            "hiring_patterns": analysis.hiring_patterns,
            "culture_indicators": analysis.culture_indicators,
            "strategic_challenges": analysis.strategic_challenges,
            "growth_indicators": analysis.growth_indicators,
            "analysis_date": analysis.analysis_date,
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/intelligence/positioning")
def develop_positioning(request: dict):
    """Develop competitive positioning strategy."""
    try:
        target_role = request.get("target_role")
        company_name = request.get("company_name")
        industry = request.get("industry")

        # Analyze company first
        company_analysis = analyze_company_strategic_needs(company_name, industry)

        # Develop positioning
        positioning = develop_competitive_positioning_strategy(target_role, company_analysis)

        return {
            "target_role": positioning.target_role,
            "key_differentiators": positioning.key_differentiators,
            "unique_value_props": positioning.unique_value_props,
            "skill_gaps_to_address": positioning.skill_gaps_to_address,
            "experience_highlights": positioning.experience_highlights,
            "competitive_threats": positioning.competitive_threats,
            "positioning_strategy": positioning.positioning_strategy,
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/intelligence/resume-targeting")
def create_resume_targeting(request: dict):
    """Create strategic resume targeting."""
    try:
        company_name = request.get("company_name")
        target_role = request.get("target_role")
        industry = request.get("industry")

        # Analyze company
        company_analysis = analyze_company_strategic_needs(company_name, industry)

        # Develop positioning
        positioning = develop_competitive_positioning_strategy(target_role, company_analysis)

        # Create targeting
        targeting = create_strategic_resume_targeting(company_analysis, positioning)

        return {
            "company_name": targeting.company_name,
            "target_role": targeting.target_role,
            "resume_focus_areas": targeting.resume_focus_areas,
            "keyword_optimization": targeting.keyword_optimization,
            "experience_prioritization": targeting.experience_prioritization,
            "skill_emphasis": targeting.skill_emphasis,
            "achievement_highlights": targeting.achievement_highlights,
            "pain_point_alignment": targeting.pain_point_alignment,
        }
    except Exception as e:
        return {"error": str(e)}
```

### TECH_DEBT_TRACKING.md
```markdown
# Technical Debt Tracking

## Security

### S314 - XML parsing in job sources

*   **Issue**: The `indeed.py` and `weworkremotely.py` job sources use insecure XML parsing, which is vulnerable to XML external entity (XXE) attacks.
*   **Mitigation**: The registration of these job sources has been temporarily disabled in `api/index.py` to make the vulnerable code paths unreachable.
*   **Next Steps**: The XML parsing in these files must be updated to use a secure parser (e.g., `defusedxml`). Once fixed, the job sources can be re-enabled.
*   **Date**: 2025-12-16

## Pre-commit Hooks

### `validate_metadata.sh`

*   **Issue**: The `validate_metadata.sh` pre-commit hook was causing issues and has been disabled.
*   **Mitigation**: The hook is not present in the `.pre-commit-config.yaml` file.
*   **Next Steps**: The hook should be re-enabled and any issues with it should be fixed. The hook is located at `scripts/validate_metadata.sh`.
*   **Date**: 2025-12-16
```

### api/self_efficacy_engine.py
```python
"""
Self-Efficacy Metrics Engine for Mosaic 2.0
Computes experiment completion, learning velocity, confidence scores, and escalation signals.
"""

import json
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, Optional, Tuple

from .settings import get_settings
from .storage import get_conn


@dataclass
class SelfEfficacyMetrics:
    """Computed self-efficacy metrics for a session"""

    session_id: str
    experiment_completion_rate: float  # 0-1
    learning_velocity: float  # learning events per day
    confidence_score: float  # 0-1
    escalation_risk: float  # 0-1 (higher = more risk)
    total_experiments: int
    completed_experiments: int
    learning_events: int
    days_active: int
    last_activity: Optional[str]
    metrics_timestamp: str


class SelfEfficacyEngine:
    """Handles self-efficacy metrics computation and escalation detection."""

    def __init__(self):
        self.settings = get_settings()
        self.metrics_enabled = self._check_feature_flag("SELF_EFFICACY_METRICS")
        self.escalation_enabled = self._check_feature_flag("COACH_ESCALATION")

    def _check_feature_flag(self, flag_name: str) -> bool:
        """Check if a feature flag is enabled."""
        try:
            with get_conn() as conn:
                row = conn.execute(
                    "SELECT enabled FROM feature_flags WHERE flag_name = ?", (flag_name,)
                ).fetchone()
                return row and row[0] if row else False
        except Exception:
            return False

    def compute_session_metrics(self, session_id: str) -> SelfEfficacyMetrics:
        """Compute comprehensive self-efficacy metrics for a session."""
        if not self.metrics_enabled:
            return SelfEfficacyMetrics(
                session_id=session_id,
                experiment_completion_rate=0.0,
                learning_velocity=0.0,
                confidence_score=0.0,
                escalation_risk=0.0,
                total_experiments=0,
                completed_experiments=0,
                learning_events=0,
                days_active=0,
                last_activity=None,
                metrics_timestamp=datetime.utcnow().isoformat(),
            )

        try:
            with get_conn() as conn:
                # Get experiment data
                experiment_rows = conn.execute(
                    """SELECT status, created_at, completed_at FROM experiments
                       WHERE session_id = ? ORDER BY created_at""",
                    (session_id,),
                ).fetchall()

                # Get learning data
                learning_rows = conn.execute(
                    """SELECT created_at, confidence_score FROM learning_data
                       WHERE session_id = ? ORDER BY created_at""",
                    (session_id,),
                ).fetchall()

                # Get self-efficacy metrics
                metrics_rows = conn.execute(
                    """SELECT metric_name, metric_value, created_at FROM self_efficacy_metrics
                       WHERE session_id = ? ORDER BY created_at""",
                    (session_id,),
                ).fetchall()

                # Compute metrics
                total_experiments = len(experiment_rows)
                completed_experiments = sum(1 for row in experiment_rows if row[0] == "completed")
                learning_events = len(learning_rows)

                # Experiment completion rate
                completion_rate = (
                    completed_experiments / total_experiments if total_experiments > 0 else 0.0
                )

                # Learning velocity (events per day)
                if learning_rows:
                    first_learning = datetime.fromisoformat(
                        learning_rows[0][0].replace("Z", "+00:00")
                    )
                    last_learning = datetime.fromisoformat(
                        learning_rows[-1][0].replace("Z", "+00:00")
                    )
                    days_active = max(1, (last_learning - first_learning).days + 1)
                    learning_velocity = learning_events / days_active
                else:
                    days_active = 0
                    learning_velocity = 0.0

                # Confidence score
                confidence_scores = [row[1] for row in learning_rows if row[1] is not None]
                avg_confidence = (
                    sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
                )

                # Escalation risk calculation
                escalation_risk = self._calculate_escalation_risk(
                    completion_rate, learning_velocity, avg_confidence, days_active
                )

                # Last activity
                last_activity = None
                if experiment_rows:
                    last_activity = experiment_rows[-1][1]  # created_at of last experiment

                return SelfEfficacyMetrics(
                    session_id=session_id,
                    experiment_completion_rate=completion_rate,
                    learning_velocity=learning_velocity,
                    confidence_score=avg_confidence,
                    escalation_risk=escalation_risk,
                    total_experiments=total_experiments,
                    completed_experiments=completed_experiments,
                    learning_events=learning_events,
                    days_active=days_active,
                    last_activity=last_activity,
                    metrics_timestamp=datetime.utcnow().isoformat(),
                )

        except Exception as e:
            print(f"Error computing metrics for session {session_id}: {e}")
            return SelfEfficacyMetrics(
                session_id=session_id,
                experiment_completion_rate=0.0,
                learning_velocity=0.0,
                confidence_score=0.0,
                escalation_risk=1.0,  # High risk if error
                total_experiments=0,
                completed_experiments=0,
                learning_events=0,
                days_active=0,
                last_activity=None,
                metrics_timestamp=datetime.utcnow().isoformat(),
            )

    def _calculate_escalation_risk(
        self, completion_rate: float, learning_velocity: float, confidence: float, days_active: int
    ) -> float:
        """Calculate escalation risk based on multiple factors."""
        risk_factors = []

        # Low completion rate increases risk
        if completion_rate < 0.3:
            risk_factors.append(0.8)
        elif completion_rate < 0.6:
            risk_factors.append(0.4)
        else:
            risk_factors.append(0.1)

        # Low learning velocity increases risk
        if learning_velocity < 0.5:
            risk_factors.append(0.7)
        elif learning_velocity < 1.0:
            risk_factors.append(0.3)
        else:
            risk_factors.append(0.1)

        # Low confidence increases risk
        if confidence < 0.3:
            risk_factors.append(0.9)
        elif confidence < 0.6:
            risk_factors.append(0.5)
        else:
            risk_factors.append(0.1)

        # Long inactivity increases risk
        if days_active > 7 and learning_velocity < 0.2:
            risk_factors.append(0.8)

        # Calculate weighted average
        return sum(risk_factors) / len(risk_factors) if risk_factors else 0.0

    def should_escalate(self, session_id: str) -> Tuple[bool, str]:
        """Determine if session should be escalated to human coach."""
        if not self.escalation_enabled:
            return False, "Escalation feature disabled"

        metrics = self.compute_session_metrics(session_id)

        # Escalation triggers
        if metrics.escalation_risk > 0.7:
            return (
                True,
                f"High escalation risk ({metrics.escalation_risk:.2f}) - low completion/confidence",
            )

        if metrics.days_active > 14 and metrics.learning_velocity < 0.1:
            return True, "Long inactivity with minimal learning progress"

        if metrics.confidence_score < 0.2 and metrics.total_experiments > 3:
            return True, "Persistently low confidence across multiple experiments"

        return False, "No escalation needed"

    def get_escalation_prompt(self, session_id: str) -> Optional[str]:
        """Get escalation prompt for coach to contact Damian."""
        should_escalate, reason = self.should_escalate(session_id)

        if not should_escalate:
            return None

        metrics = self.compute_session_metrics(session_id)

        return f"""
🚨 COACH ESCALATION NEEDED

Session: {session_id}
Reason: {reason}

Metrics Summary:
- Experiment Completion: {metrics.completed_experiments}/{metrics.total_experiments} ({metrics.experiment_completion_rate:.1%})
- Learning Velocity: {metrics.learning_velocity:.2f} events/day
- Confidence Score: {metrics.confidence_score:.2f}
- Escalation Risk: {metrics.escalation_risk:.2f}
- Days Active: {metrics.days_active}

Recommended Action: Contact Damian for personalized coaching intervention.
"""

    def cleanup_stale_experiments(self, days_threshold: int = 30) -> Dict[str, Any]:
        """Clean up stale experiments and related data."""
        cutoff_date = datetime.utcnow() - timedelta(days=days_threshold)

        try:
            with get_conn() as conn:
                # Find stale experiments
                stale_experiments = conn.execute(
                    """SELECT id FROM experiments
                       WHERE status = 'active' AND created_at < ?""",
                    (cutoff_date.isoformat(),),
                ).fetchall()

                if not stale_experiments:
                    return {"cleaned": 0, "message": "No stale experiments found"}

                # Clean up related data by iterating to avoid S608 SQL injection warnings.
                # While less performant than a single IN clause, this is demonstrably safe.
                experiment_ids = [row[0] for row in stale_experiments]

                for experiment_id in experiment_ids:
                    conn.execute(
                        "DELETE FROM learning_data WHERE experiment_id = ?",
                        (experiment_id,),
                    )
                    conn.execute("DELETE FROM experiments WHERE id = ?", (experiment_id,))

                return {
                    "cleaned": len(experiment_ids),
                    "message": f"Cleaned {len(experiment_ids)} stale experiments",
                }

        except Exception as e:
            print(f"Error computing metrics for session {session_id}: {e}")

    def record_analytics_entry(self, session_id: str, metrics: SelfEfficacyMetrics) -> None:
        """Record analytics entry for session metrics."""
        try:
            with get_conn() as conn:
                conn.execute(
                    """INSERT INTO self_efficacy_metrics
                       (session_id, metric_name, metric_value, metric_type, context_data)
                       VALUES (?, ?, ?, ?, ?)""",
                    (
                        session_id,
                        "session_analytics",
                        1.0,
                        "analytics",
                        json.dumps(
                            {
                                "experiment_completion_rate": metrics.experiment_completion_rate,
                                "learning_velocity": metrics.learning_velocity,
                                "confidence_score": metrics.confidence_score,
                                "escalation_risk": metrics.escalation_risk,
                                "total_experiments": metrics.total_experiments,
                                "completed_experiments": metrics.completed_experiments,
                                "learning_events": metrics.learning_events,
                                "days_active": metrics.days_active,
                            }
                        ),
                    ),
                )
        except Exception as e:
            print(f"Error recording analytics for session {session_id}: {e}")

    def get_health_status(self) -> Dict[str, Any]:
        """Get self-efficacy engine health status."""
        return {
            "metrics_enabled": self.metrics_enabled,
            "escalation_enabled": self.escalation_enabled,
            "feature_flags": {
                "SELF_EFFICACY_METRICS": self.metrics_enabled,
                "COACH_ESCALATION": self.escalation_enabled,
            },
            "status": "operational" if self.metrics_enabled else "disabled",
        }


# Global self-efficacy engine instance
self_efficacy_engine = SelfEfficacyEngine()


def compute_session_metrics(session_id: str) -> SelfEfficacyMetrics:
    """Compute metrics using the global engine."""
    return self_efficacy_engine.compute_session_metrics(session_id)

def should_escalate(session_id: str) -> Tuple[bool, str]:
    """Check escalation using the global engine."""
    return self_efficacy_engine.should_escalate(session_id)

def get_escalation_prompt(session_id: str) -> Optional[str]:
    """Get escalation prompt using the global engine."""
    return self_efficacy_engine.get_escalation_prompt(session_id)

def cleanup_stale_experiments(days_threshold: int = 30) -> Dict[str, Any]:
    """Cleanup using the global engine."""
    return self_efficacy_engine.cleanup_stale_experiments(days_threshold)

def record_analytics_entry(session_id: str, metrics: SelfEfficacyMetrics) -> None:
    """Record analytics using the global engine."""
    self_efficacy_engine.record_analytics_entry(session_id, metrics)

def get_self_efficacy_health() -> Dict[str, Any]:
    """Get health status using the global engine."""
    return self_efficacy_engine.get_health_status()
```

### backend/api/storage.py
```python
import json
import os
import sqlite3
import uuid
from contextlib import contextmanager
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional


DATA_ROOT = Path(os.getenv("DATA_ROOT", "data"))
DB_PATH = Path(os.getenv("DATABASE_PATH", DATA_ROOT / "mosaic.db"))
UPLOAD_ROOT = Path(os.getenv("UPLOAD_ROOT", DATA_ROOT / "uploads"))
SESSION_TTL_DAYS = int(os.getenv("SESSION_TTL_DAYS", "30"))

UPLOAD_ROOT.mkdir(parents=True, exist_ok=True)
DATA_ROOT.mkdir(parents=True, exist_ok=True)


def _json_dump(data: Any) -> str:
    return json.dumps(data, ensure_ascii=False)


def _json_load(raw: Optional[str]) -> Any:
    if raw is None:
        return None
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        return None


@contextmanager
def get_conn():
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA foreign_keys = ON")
        yield conn
        conn.commit()
    finally:
        conn.close()


def init_db() -> None:
    with get_conn() as conn:
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS sessions (
                id TEXT PRIMARY KEY,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                expires_at TIMESTAMP,
                user_data TEXT
            )
            """
        )

        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS wimd_outputs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                prompt TEXT,
                response TEXT,
                analysis_data TEXT,
                metrics TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
            )
            """
        )

        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS job_matches (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                job_id TEXT,
                company TEXT,
                role TEXT,
                fit_score REAL,
                skills_match TEXT,
                values_match TEXT,
                extras TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
            )
            """
        )

        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS resume_versions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                job_id TEXT,
                version_name TEXT,
                content TEXT,
                feedback TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
            )
            """
        )

        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS file_uploads (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                filename TEXT,
                file_path TEXT,
                file_type TEXT,
                file_size INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (session_id) REFERENCES sessions(id) ON DELETE CASCADE
            )
            """
        )


def _expiry_ts() -> datetime:
    return datetime.utcnow() + timedelta(days=SESSION_TTL_DAYS)


def create_session(user_data: Optional[Dict[str, Any]] = None) -> str:
    session_id = uuid.uuid4().hex
    payload = _json_dump(user_data or {})
    with get_conn() as conn:
        conn.execute(
            "INSERT INTO sessions (id, expires_at, user_data) VALUES (?, ?, ?)",
            (session_id, _expiry_ts(), payload),
        )
    return session_id


def ensure_session(session_id: Optional[str], user_data: Optional[Dict[str, Any]] = None) -> str:
    if not session_id:
        return create_session(user_data)
    with get_conn() as conn:
        row = conn.execute("SELECT id FROM sessions WHERE id = ?", (session_id,)).fetchone()
        if row is None:
            return create_session(user_data)
        conn.execute(
            "UPDATE sessions SET expires_at = ?, user_data = COALESCE(?, user_data) WHERE id = ?",
            (_expiry_ts(), _json_dump(user_data) if user_data is not None else None, session_id),
        )
    return session_id


def session_exists(session_id: str) -> bool:
    with get_conn() as conn:
        row = conn.execute("SELECT 1 FROM sessions WHERE id = ?", (session_id,)).fetchone()
    return row is not None


def record_wimd_output(
    session_id: str,
    prompt: str,
    response: str,
    analysis_data: Optional[Dict[str, Any]] = None,
    metrics: Optional[Dict[str, Any]] = None,
) -> None:
    with get_conn() as conn:
        conn.execute(
            """
            INSERT INTO wimd_outputs (session_id, prompt, response, analysis_data, metrics)
            VALUES (?, ?, ?, ?, ?)
            """,
            (
                session_id,
                prompt,
                response,
                _json_dump(analysis_data or {}),
                _json_dump(metrics or {}),
            ),
        )


def latest_metrics(session_id: str) -> Optional[Dict[str, Any]]:
    with get_conn() as conn:
        row = conn.execute(
            "SELECT metrics FROM wimd_outputs WHERE session_id = ? ORDER BY created_at DESC LIMIT 1",
            (session_id,),
        ).fetchone()
    if row and row["metrics"]:
        parsed = _json_load(row["metrics"])
        return parsed if isinstance(parsed, dict) else None
    return None


def wimd_history(session_id: str, limit: int = 25) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        rows = conn.execute(
            """
            SELECT prompt, response, analysis_data, metrics, created_at
            FROM wimd_outputs
            WHERE session_id = ?
            ORDER BY created_at DESC
            LIMIT ?
            """,
            (session_id, limit),
        ).fetchall()
    history: List[Dict[str, Any]] = []
    for row in rows:
        history.append(
            {
                "prompt": row["prompt"],
                "response": row["response"],
                "analysis": _json_load(row["analysis_data"]) or {},
                "metrics": _json_load(row["metrics"]) or {},
                "created_at": row["created_at"],
            }
        )
    return history


def store_job_matches(session_id: str, matches: List[Dict[str, Any]]) -> None:
    with get_conn() as conn:
        conn.execute("DELETE FROM job_matches WHERE session_id = ?", (session_id,))
        for match in matches:
            conn.execute(
                """
                INSERT INTO job_matches (session_id, job_id, company, role, fit_score, skills_match, values_match, extras)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    session_id,
                    match.get("job_id"),
                    match.get("company"),
                    match.get("role"),
                    float(match.get("fit_score", 0.0)),
                    _json_dump(match.get("skills_match", [])),
                    _json_dump(match.get("values_match", [])),
                    _json_dump(match.get("extras", {})),
                ),
            )


def fetch_job_matches(session_id: str) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        rows = conn.execute(
            "SELECT job_id, company, role, fit_score, skills_match, values_match, extras, created_at FROM job_matches WHERE session_id = ? ORDER BY fit_score DESC",
            (session_id,),
        ).fetchall()
    matches = []
    for row in rows:
        matches.append(
            {
                "job_id": row["job_id"],
                "company": row["company"],
                "role": row["role"],
                "fit_score": row["fit_score"],
                "skills_match": _json_load(row["skills_match"]) or [],
                "values_match": _json_load(row["values_match"]) or [],
                "extras": _json_load(row["extras"]) or {},
                "created_at": row["created_at"],
            }
        )
    return matches


def update_job_match_status(
    session_id: str,
    job_id: str,
    status: str,
    notes: Optional[str] = None,
) -> None:
    with get_conn() as conn:
        row = conn.execute(
            "SELECT extras FROM job_matches WHERE session_id = ? AND job_id = ?",
            (session_id, job_id),
        ).fetchone()
        if row is None:
            raise ValueError("job_match_not_found")
        extras = _json_load(row["extras"]) or {}
        extras.update(
            {
                "status": status,
                "status_notes": notes or "",
                "status_at": datetime.utcnow().isoformat() + "Z",
            }
        )
        conn.execute(
            "UPDATE job_matches SET extras = ? WHERE session_id = ? AND job_id = ?",
            (_json_dump(extras), session_id, job_id),
        )


def add_resume_version(
    session_id: str,
    version_name: str,
    content: str,
    job_id: Optional[str] = None,
    feedback: Optional[Dict[str, Any]] = None,
) -> int:
    with get_conn() as conn:
        cur = conn.execute(
            """
            INSERT INTO resume_versions (session_id, job_id, version_name, content, feedback)
            VALUES (?, ?, ?, ?, ?)
            """,
            (session_id, job_id, version_name, content, _json_dump(feedback or {})),
        )
        return cur.lastrowid


def list_resume_versions(session_id: str) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        rows = conn.execute(
            "SELECT id, job_id, version_name, content, feedback, created_at FROM resume_versions WHERE session_id = ? ORDER BY created_at DESC",
            (session_id,),
        ).fetchall()
    versions: List[Dict[str, Any]] = []
    for row in rows:
        versions.append(
            {
                "id": row["id"],
                "job_id": row["job_id"],
                "version_name": row["version_name"],
                "content": row["content"],
                "feedback": _json_load(row["feedback"]) or {},
                "created_at": row["created_at"],
            }
        )
    return versions


def store_file_upload(
    session_id: str,
    filename: str,
    file_type: str,
    file_size: int,
    file_path: Path,
) -> None:
    with get_conn() as conn:
        conn.execute(
            """
            INSERT INTO file_uploads (session_id, filename, file_path, file_type, file_size)
            VALUES (?, ?, ?, ?, ?)
            """,
            (session_id, filename, str(file_path), file_type, file_size),
        )


def list_files(session_id: str) -> List[Dict[str, Any]]:
    with get_conn() as conn:
        rows = conn.execute(
            "SELECT id, filename, file_type, file_size, created_at FROM file_uploads WHERE session_id = ? ORDER BY created_at DESC",
            (session_id,),
        ).fetchall()
    return [dict(row) for row in rows]

def cleanup_expired_sessions() -> None:
    cutoff = datetime.utcnow()
    with get_conn() as conn:
        expired_ids = [
            row[0]
            for row in conn.execute(
                "SELECT id FROM sessions WHERE expires_at <= ?", (cutoff,)
            ).fetchall()
        ]
        if expired_ids:
            placeholders = ",".join(["?"] * len(expired_ids))
            file_rows = conn.execute(
                f"SELECT file_path FROM file_uploads WHERE session_id IN ({placeholders})",  # noqa: S608
                expired_ids,
            ).fetchall()
            conn.execute(
                f"DELETE FROM file_uploads WHERE session_id IN ({placeholders})", expired_ids  # noqa: S608
            )
            conn.execute(
                f"DELETE FROM resume_versions WHERE session_id IN ({placeholders})", expired_ids  # noqa: S608
            )
            conn.execute(
                f"DELETE FROM job_matches WHERE session_id IN ({placeholders})", expired_ids  # noqa: S608
            )
            conn.execute(
                f"DELETE FROM wimd_outputs WHERE session_id IN ({placeholders})", expired_ids  # noqa: S608
            )
            conn.execute(f"DELETE FROM sessions WHERE id IN ({placeholders})", expired_ids)  # noqa: S608
            for row in file_rows:
                try:
                    path = Path(row[0])
                    if path.exists() and path.is_file():
                        path.unlink()
                except OSError:
                    pass


def session_summary(session_id: str) -> Dict[str, Any]:
    data: Dict[str, Any] = {"session_id": session_id}
    with get_conn() as conn:
        session_row = conn.execute(
            "SELECT created_at, expires_at, user_data FROM sessions WHERE id = ?",
            (session_id,),
        ).fetchone()
    if session_row:
        data["created_at"] = session_row["created_at"]
        data["expires_at"] = session_row["expires_at"]
        data["user_data"] = _json_load(session_row["user_data"]) or {}
    data["latest_metrics"] = latest_metrics(session_id) or {}
    data["files"] = list_files(session_id)
    data["resumes"] = list_resume_versions(session_id)
    data["job_matches"] = fetch_job_matches(session_id)
    return data


__all__ = [
    "UPLOAD_ROOT",
    "create_session",
    "ensure_session",
    "session_exists",
    "record_wimd_output",
    "latest_metrics",
    "wimd_history",
    "store_job_matches",
    "fetch_job_matches",
    "update_job_match_status",
    "add_resume_version",
    "list_resume_versions",
    "store_file_upload",
    "list_files",
    "cleanup_expired_sessions",
    "init_db",
    "session_summary",
]
```

### .pre-commit-config.yaml
```yaml
# Pre-commit hooks for automated code quality enforcement
# Based on ISO/IEC 5055:2021 + Policy-as-Code industry standards
# Install: pre-commit install --install-hooks
# Run manually: pre-commit run --all-files

repos:
  # Python code quality - Ruff (replaces flake8, isort, pyupgrade)
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.6.8
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
        name: Ruff linter (PEP 8 + security checks)
      - id: ruff-format
        name: Ruff formatter (Black-compatible)

  # Python code formatting - Black
  - repo: https://github.com/psf/black
    rev: 24.10.0
    hooks:
      - id: black
        name: Black code formatter
        language_version: python3.9

  # Security - Git secrets detection
  - repo: https://github.com/gitleaks/gitleaks
    rev: v8.18.4
    hooks:
      - id: gitleaks
        name: Gitleaks (detect hardcoded secrets)

  # Security - Additional secret detection
  - repo: https://github.com/Yelp/detect-secrets
    rev: v1.5.0
    hooks:
      - id: detect-secrets
        name: Detect secrets in code
        args: ['--baseline', '.secrets.baseline']
        exclude: (?x)^(?:
            \.ai-agents/handoff_20251103_101528.json|
            \.ai-agents/validation/FAILURE_MODE_TEST_RESULTS.json|
            \.ai-agents/session_context/SESSION_LOG_SCHEMA.json
        )$

  # Pre-commit hooks - General file checks
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0
    hooks:
      - id: check-added-large-files
        args: ['--maxkb=500']
        name: Check for large files (>500KB)

      - id: check-ast
        name: Check Python AST syntax

      - id: check-json
        name: Check JSON syntax
        exclude: (?x)^(?:\.ai-agents/handoff_20251103_101528.json|\.ai-agents/validation/FAILURE_MODE_TEST_RESULTS.json|\.ai-agents/session_context/SESSION_LOG_SCHEMA.json)$

      - id: check-yaml
        name: Check YAML syntax

      - id: check-toml
        name: Check TOML syntax

      - id: detect-private-key
        name: Detect private keys

      - id: end-of-file-fixer
        name: Fix end of files

      - id: trailing-whitespace
        name: Trim trailing whitespace

      - id: mixed-line-ending
        name: Fix mixed line endings
        args: ['--fix=lf']

      - id: check-merge-conflict
        name: Check for merge conflict markers

      - id: check-executables-have-shebangs
        name: Check executables have shebangs

      - id: check-shebang-scripts-are-executable
        name: Check shebang scripts are executable

  # Python security - Bandit (security linter)
  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.10
    hooks:
      - id: bandit
        name: Bandit security linter
        args: ['-c', 'pyproject.toml']
        additional_dependencies: ['bandit[toml]']

  # Python imports - isort (import sorting)
  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
        name: isort (sort imports)
        args: ['--profile', 'black']

  # Python type checking - mypy
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.11.2
    hooks:
      - id: mypy
        name: mypy (type checking)
        args: [--ignore-missing-imports, --strict]
        additional_dependencies: []
        exclude: ^tests/

  # Markdown linting
  - repo: https://github.com/igorshubovych/markdownlint-cli
    rev: v0.41.0
    hooks:
      - id: markdownlint
        language_version: system
        name: Markdown linter
        args: ['--fix']

  # Shell script linting
  - repo: https://github.com/shellcheck-py/shellcheck-py
    rev: v0.10.0.1
    hooks:
      - id: shellcheck
        name: ShellCheck (shell script linter)


# Custom local hooks
  - repo: local
    hooks:
      # Mosaic-specific: PostgreSQL pattern enforcement
      - id: check-postgres-patterns
        name: Check PostgreSQL context manager pattern
        entry: python -c "import sys; code = open(sys.argv[1]).read(); sys.exit(1 if 'conn = get_conn()' in code and 'with get_conn()' not in code else 0)"
        language: system
        files: '^api/.*\.py$'
        exclude: ^api/storage.py$

      # Mosaic-specific: No SQLite syntax in PostgreSQL code
      - id: check-no-sqlite-syntax
        name: Check no SQLite syntax (use %s not ?)
        entry: python -c "import sys, re; code = open(sys.argv[1]).read(); sys.exit(1 if re.search(r'execute\([^)]*\?', code) else 0)"
        language: system
        files: '^api/.*\.py$'


# Configuration
default_language_version:
  python: python3.9

# Global excludes
exclude: | 
  (?x)^( 
      \.git/|
      \.venv/|
      venv/|
      __pycache__/|
      \.pytest_cache/|
      node_modules/|
      build/|
      dist/|
      .*\.egg-info/|
      backups/|
      docs_archive/|
      Archives/|
      \.claude-run/
  )$

# Fail fast: stop on first failure
fail_fast: false

# Minimum pre-commit version
minimum_pre_commit_version: '2.21.0'
```

### Mosaic/PS101_Continuity_Kit/netlify.toml
```toml
# netlify.toml
# Placeholder config for PS101 continuity deployment.
```

## Executed `chmod` Commands
```bash
chmod -x scripts/capture_console.mjs
chmod +x backups/20251031_173812_render_deployment_failure/setup_claude_runner.sh
chmod +x scripts/sync_to_gdrive_oneshot.sh
chmod +x setup_claude_runner_no_conda.sh
chmod +x backups/20251031_173812_render_deployment_failure/setup_claude_runner_no_conda.sh
chmod +x scripts/agent_auto_poll.sh
chmod +x setup_claude_runner.sh
```
